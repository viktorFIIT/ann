{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Learning - Classification of birds to species with Convolutional Neural Network**\n",
    "\n",
    "Learning paradigm: (strong) supervised learning\n",
    "\n",
    "<u>This notebook documents 3. experiment conducted in 10th of April 2022</u>\n",
    "\n",
    "Dataset I work with is: https://www.kaggle.com/datasets/gpiosenka/100-bird-species/. Check what Convolutional neural networks are all about at https://d2l.ai/chapter_convolutional-neural-networks/index.html before making changes to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T19:48:30.702777Z",
     "iopub.status.busy": "2021-07-04T19:48:30.702432Z",
     "iopub.status.idle": "2021-07-04T19:48:38.452291Z",
     "shell.execute_reply": "2021-07-04T19:48:38.450907Z",
     "shell.execute_reply.started": "2021-07-04T19:48:30.702703Z"
    }
   },
   "outputs": [],
   "source": [
    "# import utilities\n",
    "import os\n",
    "import matplotlib.pyplot as plt # to evaluate model performance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf # import tensorflow\n",
    "# work with Keras facade\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Activation, Dropout # input, output, hidden layers, activation...\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D # convolutional layer and max pooling\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set consists of 58 388 RGB images (that means 3 channels), 224px x 224px. Validation set consists of 2000 images and test set consists of 2000 images. There are 356 unique bird species in a training dataset. There are 400 unique bird species in validation dataset and 400 bird species in test dataset too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T19:50:40.928323Z",
     "iopub.status.busy": "2021-07-04T19:50:40.928026Z",
     "iopub.status.idle": "2021-07-04T19:50:40.943859Z",
     "shell.execute_reply": "2021-07-04T19:50:40.943012Z",
     "shell.execute_reply.started": "2021-07-04T19:50:40.928295Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd() # extract dataset from Kaggle to same folder as you have in this notebook\n",
    "\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "VALIDATION_DIR = os.path.join(BASE_DIR, 'valid')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T19:50:45.326576Z",
     "iopub.status.busy": "2021-07-04T19:50:45.326284Z",
     "iopub.status.idle": "2021-07-04T19:50:45.362266Z",
     "shell.execute_reply": "2021-07-04T19:50:45.361629Z",
     "shell.execute_reply.started": "2021-07-04T19:50:45.326533Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CATEGORIES = os.listdir(TRAIN_DIR)\n",
    "Train_Category_count = len(TRAIN_CATEGORIES) # gets you number of classes in training dataset\n",
    "\n",
    "VAL_CATEGORIES = os.listdir(VALIDATION_DIR)\n",
    "Val_Category_count = len(VAL_CATEGORIES)\n",
    "\n",
    "TEST_CATEGORIES = os.listdir(TEST_DIR)\n",
    "Test_Category_count = len(TEST_CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying standard rescale factor by which all data values would be multiplied. We're doing this because we deal with images in RGB color model, where pixel values vary between 0 and 255. Such values would be too high for our model to process. This is why I rescale them to interval 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T19:51:00.728664Z",
     "iopub.status.busy": "2021-07-04T19:51:00.728356Z",
     "iopub.status.idle": "2021-07-04T19:51:00.732946Z",
     "shell.execute_reply": "2021-07-04T19:51:00.731797Z",
     "shell.execute_reply.started": "2021-07-04T19:51:00.728632Z"
    }
   },
   "outputs": [],
   "source": [
    "data_iterator = ImageDataGenerator(rescale=1./255,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T19:51:04.169Z",
     "iopub.status.busy": "2021-07-04T19:51:04.168706Z",
     "iopub.status.idle": "2021-07-04T19:51:11.182658Z",
     "shell.execute_reply": "2021-07-04T19:51:11.180853Z",
     "shell.execute_reply.started": "2021-07-04T19:51:04.168973Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = data_iterator.flow_from_directory(\n",
    "    directory = TRAIN_DIR, \n",
    "    batch_size = 32, \n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224,224))\n",
    "\n",
    "validation_data = data_iterator.flow_from_directory(\n",
    "    directory = VALIDATION_DIR, \n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224))\n",
    "\n",
    "test_data = data_iterator.flow_from_directory(\n",
    "    directory = TEST_DIR,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture of this Convolutional Neural Network**\n",
    "\n",
    "1. Convolutional layer - to extract patterns and abstract from low-level features of the images\n",
    "2. Activation layer - activation functions with a same purpose as they have in Multilayer perceptrons. \n",
    "3. Pooling layer - optimization with Max pooling.\n",
    "4. Normalization layer - batch normalization as optimization technique. Conducted after activation, before another convolution\n",
    "5. Dense layer - fully connected layer of the MLP.\n",
    "\n",
    "Input to this convolutional network are RGB images 224 x 224 px with 3 channels. First convolutional layer works with 3 channels, but this does not mean all convolutional layers have to work with these same 3 channels (they usually create activation maps with more channels).\n",
    "\n",
    "Best practice is to use same activation function across all layers. \n",
    "\n",
    "**Strategy of my Learning Process**\n",
    "\n",
    "1. Setup, build and run my Convolutional Neural Network.\n",
    "2. Check my model performance.\n",
    "3. Conduct more experiments with probably different set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T19:51:29.362081Z",
     "iopub.status.busy": "2021-07-04T19:51:29.361815Z",
     "iopub.status.idle": "2021-07-04T20:52:37.634659Z",
     "shell.execute_reply": "2021-07-04T20:52:37.633894Z",
     "shell.execute_reply.started": "2021-07-04T19:51:29.362054Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "IMAGE = load_img(os.getcwd() + \"\\\\testing\\\\ABBOTTS BABBLER\\\\1.jpg\")\n",
    "IMAGEDATA = img_to_array(IMAGE)\n",
    "SHAPE = IMAGEDATA.shape\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# lowest convolutional layer for identification of the edges of birds\n",
    "# input shape is provided, so no deferred initialization https://d2l.ai/chapter_deep-learning-computation/deferred-init.html\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=SHAPE))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# convolutional layer to learn and store mid-level features of the bird species\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "# highest convolutional layer to store complex information about the look of birds\n",
    "model.add(Conv2D(64, (3, 3), padding='same')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same')) #54x54\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# and finally mlp\n",
    "model.add(Flatten()) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(512)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(Train_Category_count)) \n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Learning**\n",
    "\n",
    "To summarize, in this experiment, task here was to increase accuracy of the model from experiment one and two. In order to do that, we:\n",
    "\n",
    "1. we increased number of filters in the first convolutional layer from 32 (used in first two experiments) to 64. Reason to do that we were trying to increase its computational power. <br>\n",
    "2. for a same reason, in order to increase model computational power we were adding here two new convolutional layers (with a same number of convolutional filters). <br>\n",
    "3. in order to increase speed of the learning and enhance our learning process we're using here technique of batch normalization. This means we're normalizing output from all previous layers passed to the next layers, which also helped to generalize. <br>\n",
    "4. we're also using here one regularization technique: dropout, which was expected to temporarily halt impact of some neurons on layers which would have inadequately large weights for its neurons (being compared to others). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 32 # this batch size allows us to perform normalization after each activation layer \n",
    "\n",
    "history = model.fit(train_data, epochs=EPOCHS, validation_data = validation_data, \n",
    "                    steps_per_epoch=len(train_data), validation_steps = len(validation_data), \n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights = True),\n",
    "                    ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, \n",
    "                                 patience = 2, verbose = 1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T20:52:37.636866Z",
     "iopub.status.busy": "2021-07-04T20:52:37.636632Z",
     "iopub.status.idle": "2021-07-04T20:52:49.030372Z",
     "shell.execute_reply": "2021-07-04T20:52:49.029137Z",
     "shell.execute_reply.started": "2021-07-04T20:52:37.636838Z"
    }
   },
   "source": [
    "**Model Evaluation**\n",
    "\n",
    "This network was one of the best in all experiments which I've conducted. It achieved more than 80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "scores = model.evaluate(test_data, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating Model Performance**\n",
    "\n",
    "See attached document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
