{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8202b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa57f06",
   "metadata": {},
   "source": [
    "Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modules = OrderedDict()\n",
    "        self._parameters = OrderedDict()\n",
    "\n",
    "    def add_module(self, module, name:str):\n",
    "        if hasattr(self, name) and name not in self.modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self.modules[name] = module\n",
    "\n",
    "    def register_parameter(self, name, param):\n",
    "        if '.' in name:\n",
    "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        else:\n",
    "            self._parameters[name] = param\n",
    "\n",
    "    def parameters(self, recurse=True):\n",
    "        for name, param in self._parameters.items():\n",
    "            if param.requires_grad:\n",
    "                yield name, param\n",
    "        if recurse:\n",
    "            for name, module in self._modules.items():\n",
    "                for name, param in module.parameters(recurse):\n",
    "                    if param.requires_grad:\n",
    "                        yield name, param\n",
    "\n",
    "    def __dir__(self):\n",
    "        module_attrs = dir(self.__class__)\n",
    "        attrs = list(self.__dict__.keys())\n",
    "        modules = list(self._modules.keys())\n",
    "        parameters = list(self._parameters.keys())\n",
    "        keys = module_attrs + attrs + modules + parameters\n",
    "\n",
    "        # Eliminate attrs that are not legal Python variable names\n",
    "        keys = [key for key in keys if not key[0].isdigit()]\n",
    "\n",
    "        return sorted(keys)\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        if '_modules' in self.__dict__:\n",
    "            modules = self.__dict__['_modules']\n",
    "            if name in modules:\n",
    "                return modules[name]\n",
    "        if '_parameters' in self.__dict__:\n",
    "            parameters = self.__dict__['_parameters']\n",
    "            if name in parameters:\n",
    "                return parameters[name]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, name))\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Module):\n",
    "            self._modules[name] = value\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            self.register_parameter(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd27ba8",
   "metadata": {},
   "source": [
    "Simplest implementation of MLP supporting forward and backward pass for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526fbe99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModel\u001b[39;00m(\u001b[43mModule\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Model(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6cd5c",
   "metadata": {},
   "source": [
    "Loss functions supporting forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282788ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.square(np.subtract(target, input)).mean()\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return (-2*(target-input))\n",
    "\n",
    "\n",
    "class BCELoss(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -(np.multiply(target, np.log(input))+np.multiply((1-target), np.log(1-input)))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -target / input  + (1 - target) / (1 - input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8a8de",
   "metadata": {},
   "source": [
    "3 most commonly used activation functions for ANN implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ddf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        z = self.forward(self.fw_input) \n",
    "        return np.multiply(da, np.multiply(z, 1 - z)) \n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2*input) - 1) / (np.exp(2*input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        return np.multiply(da, 1-np.square(self.forward(self.fw_input)))\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return np.maximum(input, 0.0) \n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        return np.multiply(da, np.where(self.fw_input>0, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d07069",
   "metadata": {},
   "source": [
    "Input, Hidden and Output layer for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "     \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.b = np.zeros((out_features, 1)) # Watch-out for the shape\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_inputs = input\n",
    "        self.m = self.fw_inputs.shape[1]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        da = self.W.T @ dz\n",
    "        self.dW = (1/self.m)*np.matmul(dz, self.fw_inputs.T)\n",
    "        self.db = (1/self.m)*np.sum(dz, axis = 1, keepdims=True)\n",
    "        return da\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459fd7b",
   "metadata": {},
   "source": [
    "Utility function to create sample dataset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51097235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_Circles(m=10, radius=0.7, noise=0.0, verbose=False):\n",
    "\n",
    "    X = (np.random.rand(2, m) * 2.0) - 1.0\n",
    "    if (verbose): print('X: \\n', X, '\\n')\n",
    "\n",
    "    N = (np.random.rand(2, m)-0.5) * noise\n",
    "    if (verbose): print('N: \\n', N, '\\n')\n",
    "    Xnoise = X + N\n",
    "    if (verbose): print('Xnoise: \\n', Xnoise, '\\n')\n",
    "\n",
    "    XSquare = Xnoise ** 2\n",
    "    if (verbose): print('XSquare: \\n', XSquare, '\\n')\n",
    "\n",
    "    RSquare = np.sum(XSquare, axis=0, keepdims=True)\n",
    "    if (verbose): print('RSquare: \\n', RSquare, '\\n')\n",
    "    R = np.sqrt(RSquare)\n",
    "    if (verbose): print('R: \\n', R, '\\n')\n",
    "\n",
    "    Y = (R > radius).astype(float)\n",
    "    if (verbose): print('Y: \\n', Y, '\\n')\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87177b",
   "metadata": {},
   "source": [
    "Utility function for MLP learning process evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d14543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(network:Module, loss_function:Module, X:np.ndarray, Y:np.ndarray, epsilon=1e-7):\n",
    "    gradapprox = []\n",
    "    grad_backward = []\n",
    "\n",
    "    for name, layer in network.modules.items():\n",
    "        # Compute gradapprox\n",
    "        if not hasattr(layer, \"W\"):\n",
    "            continue\n",
    "        if not hasattr(layer, \"dW\"):\n",
    "            continue\n",
    "        shape = layer.W.shape\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                origin_W = np.copy(layer.W[i][j])\n",
    "\n",
    "                layer.W[i][j] = origin_W + epsilon\n",
    "                A_plus = network(X)\n",
    "                J_plus = np.mean(loss_function(A_plus, Y))\n",
    "\n",
    "                layer.W[i][j] = origin_W - epsilon\n",
    "                A_minus = network(X)\n",
    "                J_minus = np.mean(loss_function(A_minus, Y))\n",
    "\n",
    "                # Compute gradapprox[i]\n",
    "                gradapprox.append((J_plus - J_minus) / (2 * epsilon))\n",
    "                grad_backward.append(layer.dW[i][j])\n",
    "                layer.W[i][j] = origin_W\n",
    "\n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    gradapprox = np.reshape(gradapprox, (-1, 1))\n",
    "    grad_backward = np.reshape(grad_backward, (-1, 1))\n",
    "\n",
    "    numerator = np.linalg.norm(grad_backward - gradapprox)\n",
    "    denominator = np.linalg.norm(grad_backward) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 2e-7 or not difference:\n",
    "        print(\n",
    "            \"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31df02c",
   "metadata": {},
   "source": [
    "Perform and evaluate forward and backward pass through MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef637868",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features_X, dataset_labels_Y = dataset_Circles(m=128, radius=0.7, noise=0.0)\n",
    "\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 20), 'first-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-1')\n",
    "mlp.add_module(Linear(20, 20), 'second-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-2')\n",
    "mlp.add_module(Linear(20, 20), 'third-hidden')\n",
    "mlp.add_module(Tanh(), 'activation-3')\n",
    "mlp.add_module(Linear(20, 20), 'fourth-hidden')\n",
    "mlp.add_module(ReLU(), 'activation-4')\n",
    "mlp.add_module(Linear(20, 20), 'fifth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-5')\n",
    "mlp.add_module(Linear(20, 1), 'sixth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-6')\n",
    "\n",
    "y_hat = mlp.forward(dataset_features_X)\n",
    "\n",
    "bce = BCELoss()\n",
    "loss = bce.forward(y_hat, dataset_labels_Y)\n",
    "\n",
    "back = bce.backward(y_hat, dataset_labels_Y)\n",
    "\n",
    "output = mlp.backward(back)\n",
    "\n",
    "(gradient_check(mlp, bce, dataset_features_X, dataset_labels_Y))\n",
    "\n",
    "\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 20), 'first-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-1')\n",
    "mlp.add_module(Linear(20, 20), 'second-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-2')\n",
    "mlp.add_module(Linear(20, 20), 'third-hidden')\n",
    "mlp.add_module(Tanh(), 'activation-3')\n",
    "mlp.add_module(Linear(20, 20), 'fourth-hidden')\n",
    "mlp.add_module(ReLU(), 'activation-4')\n",
    "mlp.add_module(Linear(20, 20), 'fifth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-5')\n",
    "mlp.add_module(Linear(20, 1), 'sixth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-6')\n",
    "\n",
    "y_hat = mlp.forward(dataset_features_X)\n",
    "\n",
    "mse = MSELoss()\n",
    "loss = mse.forward(y_hat, dataset_labels_Y)\n",
    "\n",
    "back = mse.backward(y_hat, dataset_labels_Y)\n",
    "\n",
    "output = mlp.backward(back)\n",
    "\n",
    "(gradient_check(mlp, mse, dataset_features_X, dataset_labels_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
