{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8202b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fcaa6a",
   "metadata": {},
   "source": [
    "Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e647633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modules = OrderedDict()\n",
    "        self._parameters = OrderedDict()\n",
    "\n",
    "    def add_module(self, module, name:str):\n",
    "        if hasattr(self, name) and name not in self.modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self.modules[name] = module\n",
    "\n",
    "    def register_parameter(self, name, param):\n",
    "        if '.' in name:\n",
    "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        else:\n",
    "            self._parameters[name] = param\n",
    "\n",
    "    def parameters(self, recurse=True):\n",
    "        for name, param in self._parameters.items():\n",
    "            if param.requires_grad:\n",
    "                yield name, param\n",
    "        if recurse:\n",
    "            for name, module in self._modules.items():\n",
    "                for name, param in module.parameters(recurse):\n",
    "                    if param.requires_grad:\n",
    "                        yield name, param\n",
    "\n",
    "    def __dir__(self):\n",
    "        module_attrs = dir(self.__class__)\n",
    "        attrs = list(self.__dict__.keys())\n",
    "        modules = list(self._modules.keys())\n",
    "        parameters = list(self._parameters.keys())\n",
    "        keys = module_attrs + attrs + modules + parameters\n",
    "\n",
    "        # Eliminate attrs that are not legal Python variable names\n",
    "        keys = [key for key in keys if not key[0].isdigit()]\n",
    "\n",
    "        return sorted(keys)\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        if '_modules' in self.__dict__:\n",
    "            modules = self.__dict__['_modules']\n",
    "            if name in modules:\n",
    "                return modules[name]\n",
    "        if '_parameters' in self.__dict__:\n",
    "            parameters = self.__dict__['_parameters']\n",
    "            if name in parameters:\n",
    "                return parameters[name]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, name))\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Module):\n",
    "            self._modules[name] = value\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            self.register_parameter(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070fc5f1",
   "metadata": {},
   "source": [
    "Simplest implementation of MLP supporting forward and backward pass for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f465db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1bc16",
   "metadata": {},
   "source": [
    "Loss functions supporting forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4345cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.square(np.subtract(target, input)).mean()\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return (-2*(target-input))\n",
    "\n",
    "\n",
    "class BCELoss(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -(np.multiply(target, np.log(input))+np.multiply((1-target), np.log(1-input)))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -target / input  + (1 - target) / (1 - input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38f299",
   "metadata": {},
   "source": [
    "3 most commonly used activation functions for ANN implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59da5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        z = self.forward(self.fw_input) \n",
    "        return np.multiply(da, np.multiply(z, 1 - z)) \n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2*input) - 1) / (np.exp(2*input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        return np.multiply(da, 1-np.square(self.forward(self.fw_input)))\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return np.maximum(input, 0.0) \n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        return np.multiply(da, np.where(self.fw_input>0, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d440bc",
   "metadata": {},
   "source": [
    "Input, Hidden and Output layer for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a5db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "     \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.b = np.zeros((out_features, 1)) # Watch-out for the shape\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_inputs = input\n",
    "        self.m = self.fw_inputs.shape[1]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        da = self.W.T @ dz\n",
    "        self.dW = (1/self.m)*np.matmul(dz, self.fw_inputs.T)\n",
    "        self.db = (1/self.m)*np.sum(dz, axis = 1, keepdims=True)\n",
    "        return da\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82710b8",
   "metadata": {},
   "source": [
    "Utility function to create sample dataset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aabba045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_Circles(m=10, radius=0.7, noise=0.0, verbose=False):\n",
    "\n",
    "    X = (np.random.rand(2, m) * 2.0) - 1.0\n",
    "    if (verbose): print('X: \\n', X, '\\n')\n",
    "\n",
    "    N = (np.random.rand(2, m)-0.5) * noise\n",
    "    if (verbose): print('N: \\n', N, '\\n')\n",
    "    Xnoise = X + N\n",
    "    if (verbose): print('Xnoise: \\n', Xnoise, '\\n')\n",
    "\n",
    "    XSquare = Xnoise ** 2\n",
    "    if (verbose): print('XSquare: \\n', XSquare, '\\n')\n",
    "\n",
    "    RSquare = np.sum(XSquare, axis=0, keepdims=True)\n",
    "    if (verbose): print('RSquare: \\n', RSquare, '\\n')\n",
    "    R = np.sqrt(RSquare)\n",
    "    if (verbose): print('R: \\n', R, '\\n')\n",
    "\n",
    "    Y = (R > radius).astype(float)\n",
    "    if (verbose): print('Y: \\n', Y, '\\n')\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da115cea",
   "metadata": {},
   "source": [
    "Utility function for MLP learning process evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f089514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(network:Module, loss_function:Module, X:np.ndarray, Y:np.ndarray, epsilon=1e-7):\n",
    "    gradapprox = []\n",
    "    grad_backward = []\n",
    "\n",
    "    for name, layer in network.modules.items():\n",
    "        # Compute gradapprox\n",
    "        if not hasattr(layer, \"W\"):\n",
    "            continue\n",
    "        if not hasattr(layer, \"dW\"):\n",
    "            continue\n",
    "        shape = layer.W.shape\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                origin_W = np.copy(layer.W[i][j])\n",
    "\n",
    "                layer.W[i][j] = origin_W + epsilon\n",
    "                A_plus = network(X)\n",
    "                J_plus = np.mean(loss_function(A_plus, Y))\n",
    "\n",
    "                layer.W[i][j] = origin_W - epsilon\n",
    "                A_minus = network(X)\n",
    "                J_minus = np.mean(loss_function(A_minus, Y))\n",
    "\n",
    "                # Compute gradapprox[i]\n",
    "                gradapprox.append((J_plus - J_minus) / (2 * epsilon))\n",
    "                grad_backward.append(layer.dW[i][j])\n",
    "                layer.W[i][j] = origin_W\n",
    "\n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    gradapprox = np.reshape(gradapprox, (-1, 1))\n",
    "    grad_backward = np.reshape(grad_backward, (-1, 1))\n",
    "\n",
    "    numerator = np.linalg.norm(grad_backward - gradapprox)\n",
    "    denominator = np.linalg.norm(grad_backward) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 2e-7 or not difference:\n",
    "        print(\n",
    "            \"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da4963f",
   "metadata": {},
   "source": [
    "Perform and evaluate forward and backward pass through MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e139469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.2071803817734068e-08\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.8512093632985954e-09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset_features_X, dataset_labels_Y = dataset_Circles(m=128, radius=0.7, noise=0.0)\n",
    "\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 20), 'first-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-1')\n",
    "mlp.add_module(Linear(20, 20), 'second-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-2')\n",
    "mlp.add_module(Linear(20, 20), 'third-hidden')\n",
    "mlp.add_module(Tanh(), 'activation-3')\n",
    "mlp.add_module(Linear(20, 20), 'fourth-hidden')\n",
    "mlp.add_module(ReLU(), 'activation-4')\n",
    "mlp.add_module(Linear(20, 20), 'fifth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-5')\n",
    "mlp.add_module(Linear(20, 1), 'sixth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-6')\n",
    "\n",
    "y_hat = mlp.forward(dataset_features_X)\n",
    "\n",
    "bce = BCELoss()\n",
    "loss = bce.forward(y_hat, dataset_labels_Y)\n",
    "\n",
    "back = bce.backward(y_hat, dataset_labels_Y)\n",
    "\n",
    "output = mlp.backward(back)\n",
    "\n",
    "(gradient_check(mlp, bce, dataset_features_X, dataset_labels_Y))\n",
    "\n",
    "\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 20), 'first-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-1')\n",
    "mlp.add_module(Linear(20, 20), 'second-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-2')\n",
    "mlp.add_module(Linear(20, 20), 'third-hidden')\n",
    "mlp.add_module(Tanh(), 'activation-3')\n",
    "mlp.add_module(Linear(20, 20), 'fourth-hidden')\n",
    "mlp.add_module(ReLU(), 'activation-4')\n",
    "mlp.add_module(Linear(20, 20), 'fifth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-5')\n",
    "mlp.add_module(Linear(20, 1), 'sixth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-6')\n",
    "\n",
    "y_hat = mlp.forward(dataset_features_X)\n",
    "\n",
    "mse = MSELoss()\n",
    "loss = mse.forward(y_hat, dataset_labels_Y)\n",
    "\n",
    "back = mse.backward(y_hat, dataset_labels_Y)\n",
    "\n",
    "output = mlp.backward(back)\n",
    "\n",
    "(gradient_check(mlp, mse, dataset_features_X, dataset_labels_Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
